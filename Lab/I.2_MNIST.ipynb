{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "387810fc-4e84-4f7e-8cb2-450a79f91e08",
   "metadata": {},
   "source": [
    "# MNIST dataset - Classifiying hand-written digits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35646e6a",
   "metadata": {},
   "source": [
    "Let's start by loading libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fda02c12-6b7c-4cbf-b3b6-430e4fdd389e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax \n",
    "import jax.numpy as jnp \n",
    "import flax.linen as nn \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optax\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bebc967-ea8a-4d20-82cb-c31b2b1ae02d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load data\n",
    "digits = load_digits()\n",
    "Images = digits.data\n",
    "target = digits.target\n",
    "#visualize\n",
    "plt.gray()\n",
    "plt.matshow(digits.images[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9a53b7c-3f2d-4543-b79d-873d3ad58b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-layer Perceptron\n",
    "    \n",
    "    This class represents a Multi-layer Perceptron (MLP) model, which is a type of feedforward neural network.\n",
    "    MLPs consist of multiple layers of interconnected nodes (called neurons) and are commonly used for\n",
    "    classification tasks.\n",
    "    \n",
    "    Parameters:\n",
    "        nhidden_units (int): Number of hidden units in each layer.\n",
    "        nlayers (int, optional): Number of layers in the MLP. Defaults to 1.\n",
    "        nclasses (int, optional): Number of output classes (categories). Defaults to 10.\n",
    "    \n",
    "    Attributes:\n",
    "        layers (list): List of Dense layers representing the hidden layers in the MLP.\n",
    "        final_layer (Dense): Final Dense layer for producing the output of the MLP.\n",
    "    \"\"\"\n",
    "    nhidden_units: int\n",
    "    nlayers = 1\n",
    "    nclasses = 10\n",
    "    \n",
    "    def setup(self):\n",
    "        \"\"\"\n",
    "        Setup the MLP model by creating the hidden layers and the final output layer.\n",
    "        \"\"\"\n",
    "        self.layers = [nn.Dense(self.nhidden_units) for n in range(self.nlayers)]\n",
    "        self.final_layer = nn.Dense(self.nclasses)\n",
    "        \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Perform a forward pass through the MLP.\n",
    "        \n",
    "        Args:\n",
    "            x (jax.interpreters.xla.DeviceArray): Input data.\n",
    "            \n",
    "        Returns:\n",
    "            jax.interpreters.xla.DeviceArray: Output of the MLP.\n",
    "        \"\"\"\n",
    "        for k, lyr in enumerate(self.layers):\n",
    "            x = lyr(x)\n",
    "            x = nn.relu(x)\n",
    "        x = self.final_layer(x)\n",
    "        x = nn.activation.softmax(x)\n",
    "        return x\n",
    "# initializing the optimizer\n",
    "learning_rate = 1e-3\n",
    "optx = optax.adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e902b185-9ca8-491f-b8bf-ddee2df6b090",
   "metadata": {},
   "source": [
    "The target values we loaded are integers. We now write a code to one-hot encode them. For example, the category \"1\" would become [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5540928-b997-4439-b46e-20a38a8b4b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target value: 8, corresponding one-hot vector: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encoder(x):\n",
    "    \"\"\"\n",
    "    One-Hot Encoder\n",
    "    \n",
    "    Converts an array of categorical labels to one-hot encoded representation.\n",
    "    \n",
    "    Args:\n",
    "        x (ndarray): Array of categorical labels.\n",
    "        \n",
    "    Returns:\n",
    "        ndarray: One-hot encoded representation of the input labels.\n",
    "    \"\"\"\n",
    "    nclasses = 10  # Number of classes/categories\n",
    "    out = np.zeros((x.shape[0], nclasses))  # Initialize an array of zeros with shape (number of samples, number of classes)\n",
    "    \n",
    "    for i, x_ in enumerate(x):\n",
    "        out[i, x_] = 1.  # Set the corresponding class index to 1 in each sample\n",
    "        \n",
    "    return out\n",
    "target_oh = one_hot_encoder(target)\n",
    "# test\n",
    "print(f\"target value: {target[40]}, corresponding one-hot vector: {target_oh[40,:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a468aa-d570-4492-ab8f-fc8deed40bc9",
   "metadata": {},
   "source": [
    "## Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ab7ee2f-1bea-4479-9641-7c2d0b674fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss function: 7.451040744781494\n",
      "epoch: 100, loss function: 0.03282686695456505\n",
      "epoch: 200, loss function: 0.00741676427423954\n",
      "epoch: 300, loss function: 0.003186935791745782\n",
      "epoch: 400, loss function: 0.0017354247393086553\n",
      "epoch: 500, loss function: 0.001089397701434791\n",
      "epoch: 600, loss function: 0.0007527723209932446\n",
      "epoch: 700, loss function: 0.0005527461180463433\n",
      "epoch: 800, loss function: 0.0004231222264934331\n",
      "epoch: 900, loss function: 0.00033434192300774157\n"
     ]
    }
   ],
   "source": [
    "# split the data into training and test data\n",
    "x_train, x_test, y_train, y_test = train_test_split(Images, target_oh, test_size=0.33, random_state=42)\n",
    "# initialize the neural network \n",
    "model = MLP(256)\n",
    "params = model.init(jax.random.PRNGKey(0), x_test)\n",
    "opt_state = optx.init(params)\n",
    "\n",
    "# define the loss function \n",
    "@jax.jit\n",
    "def cat_cross_entropy(params, x, y_true):\n",
    "    \"\"\"\n",
    "    Categorical Cross Entropy Loss Function\n",
    "\n",
    "    This function calculates the categorical cross entropy loss between the predicted\n",
    "    probabilities and the true one-hot encoded labels.\n",
    "\n",
    "    Parameters:\n",
    "        params (jax.interpreters.xla.DeviceArray): Model parameters.\n",
    "        x (jax.interpreters.xla.DeviceArray): Input data.\n",
    "        y_true (jax.interpreters.xla.DeviceArray): True one-hot encoded labels.\n",
    "\n",
    "    Returns:\n",
    "        jax.interpreters.xla.DeviceArray: Mean categorical cross entropy loss.\n",
    "    \"\"\"\n",
    "    # Forward pass to obtain predicted probabilities\n",
    "    y_pred = model.apply(params, x)\n",
    "\n",
    "    # Calculation of the loss per example\n",
    "    loss_per_example = jnp.sum(-y_true * jnp.log(y_pred), axis=1)\n",
    "\n",
    "    # Return the mean loss across all examples\n",
    "    return jnp.mean(loss_per_example)\n",
    "\n",
    "loss_fn = cat_cross_entropy\n",
    "loss_grad_fn = jax.value_and_grad(cat_cross_entropy) # a function to evaluate the function and its gradient)\n",
    "\n",
    "# training loop\n",
    "n_epochs = 1000 #number of training epochs \n",
    "for e in range(n_epochs):\n",
    "    loss_val, grad = loss_grad_fn(params, x_train, y_train)\n",
    "    updates, opt_state = optx.update(grad, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    if e % 100 == 0:\n",
    "        print(f\"epoch: {e}, loss function: {loss_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e61faf-3a3d-4abb-9edc-954241271303",
   "metadata": {},
   "source": [
    "Test the learned function on some random input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbca75e5-2ec6-4584-b5ba-195279d6fdcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ture value: 6 predicted: 6\n"
     ]
    }
   ],
   "source": [
    "sample = np.random.randint(low=0, high=x_test.shape[0], size=(1,))\n",
    "y_pred = np.argmax(model.apply(params, x_test[sample,:]))\n",
    "print(f\"ture value: {np.argmax(y_test[sample,:])} predicted: {y_pred}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6352a57e-0e56-47b1-b514-2249afd66b2f",
   "metadata": {},
   "source": [
    "Evaluate the training and test errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "497b3640-7462-47fd-b1cd-c5e5e5dde919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.00027058509294874966, test loss: 0.09261590987443924\n"
     ]
    }
   ],
   "source": [
    "tr_loss = cat_cross_entropy(params, x_train, y_train)\n",
    "te_loss = cat_cross_entropy(params, x_test, y_test)\n",
    "print(f\"training loss: {tr_loss}, test loss: {te_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9256a74b-3c1e-45fd-81e7-d4f890756116",
   "metadata": {},
   "source": [
    "We now evaluate the training and test error. Does the model suffer from overfitting? from underfitting? from both?\n",
    "\n",
    "Experiment with the following parameters and see how that affects over-/underfitting. \n",
    "\n",
    "- Complexity of the neural network; by increasing the number of hidden units or number of layers.\n",
    "- Add a regularization layer; it prevents the weights of the neural network from becoming so big. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
